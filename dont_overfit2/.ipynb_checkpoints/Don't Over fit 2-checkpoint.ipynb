{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv  test.csv  test.csv.zip  train.csv\r\n"
     ]
    }
   ],
   "source": [
    "! ls ~/data/kaggle/dont-overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"~/data/kaggle/dont-overfit/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns=['target']).values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(x)\n",
    "std = np.std(x)\n",
    "\n",
    "def normalize(X):\n",
    "    return (X-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DofDataset(Dataset):\n",
    "    def __init__(self,df, test=False):\n",
    "        self.df = df\n",
    "        self.test = test\n",
    "            \n",
    "    @property\n",
    "    def tensor_labels(self):\n",
    "        return torch.from_numpy(self.df.target.values).long()\n",
    "    \n",
    "    @property\n",
    "    def tensor_data(self): #need to normalise data\n",
    "        if self.test:\n",
    "            return torch.from_numpy(self.df.values).float()\n",
    "        return torch.from_numpy(df.drop(columns=['target']).values).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        if self.test:\n",
    "            return self.tensor_data[idx]\n",
    "        label = self.tensor_labels[idx]\n",
    "        data = self.tensor_data[idx]\n",
    "        \n",
    "        return label, data\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean = np.mean(X_train)\n",
    "# std = np.std(X_train)\n",
    "\n",
    "# def standardize(x):\n",
    "#     return (x-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dod_ds = DofDataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def accuracy(preds,labels):\n",
    "#     preds_np = np.argmax(preds.detach().numpy(), axis=1)\n",
    "#     return np.sum(preds_np == labels.detach().numpy())/len(preds_np)\n",
    "\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = 301\n",
    "out_size = 2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_size, 300, bias=True),\n",
    "    nn.SELU(),\n",
    "    nn.Linear(300, 300, bias=True),\n",
    "    nn.SELU(),\n",
    "    nn.Linear(300, 300, bias=True),\n",
    "    nn.SELU(),\n",
    "    nn.Linear(300, 200, bias=True),\n",
    "    nn.SELU(),\n",
    "    nn.Linear(200, out_size, bias=True))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(in_size, 2, bias=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=301, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "bs = 50\n",
    "\n",
    "cross_en = nn.CrossEntropyLoss()\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "dod_df = DataLoader(dod_ds, bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch, train):\n",
    "    labels, data = batch\n",
    "    data_norm = normalize(data)\n",
    "    preds = model(data_norm)\n",
    "    loss = cross_en(preds, labels)\n",
    "    acc = accuracy(preds,labels)\n",
    "    if train:\n",
    "        optim.zero_grad() # zero gradients (?? why do we do this)\n",
    "        loss.backward() # calcuate gradients\n",
    "        optim.step() # updated weights\n",
    "    return loss.item(), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_t = []\n",
    "train_acc_t = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 Train Loss:  3.39088134765625, Accuracy 0.6359999775886536\n",
      "epoch:1 Train Loss:  3.385093021392822, Accuracy 0.6360000371932983\n",
      "epoch:2 Train Loss:  3.3799693107604982, Accuracy 0.6360000371932983\n",
      "epoch:3 Train Loss:  3.3741981029510497, Accuracy 0.6359999775886536\n",
      "epoch:4 Train Loss:  3.368836784362793, Accuracy 0.6359999775886536\n",
      "epoch:5 Train Loss:  3.36356782913208, Accuracy 0.6360000371932983\n",
      "epoch:6 Train Loss:  3.358321189880371, Accuracy 0.6360000371932983\n",
      "epoch:7 Train Loss:  3.352510690689087, Accuracy 0.6359999775886536\n",
      "epoch:8 Train Loss:  3.3470391750335695, Accuracy 0.6360000371932983\n",
      "epoch:9 Train Loss:  3.341892862319946, Accuracy 0.6359999775886536\n",
      "epoch:10 Train Loss:  3.3365218162536623, Accuracy 0.6360000371932983\n",
      "epoch:11 Train Loss:  3.330832672119141, Accuracy 0.6359999775886536\n",
      "epoch:12 Train Loss:  3.3257226943969727, Accuracy 0.6360000371932983\n",
      "epoch:13 Train Loss:  3.320376682281494, Accuracy 0.6359999775886536\n",
      "epoch:14 Train Loss:  3.3148202419281008, Accuracy 0.6360000371932983\n",
      "epoch:15 Train Loss:  3.3094294548034666, Accuracy 0.6360000371932983\n",
      "epoch:16 Train Loss:  3.304133749008179, Accuracy 0.6359999775886536\n",
      "epoch:17 Train Loss:  3.298775386810303, Accuracy 0.6360000371932983\n",
      "epoch:18 Train Loss:  3.2932826042175294, Accuracy 0.6360000371932983\n",
      "epoch:19 Train Loss:  3.2880776643753054, Accuracy 0.6360000371932983\n",
      "epoch:20 Train Loss:  3.2823123931884766, Accuracy 0.6359999775886536\n",
      "epoch:21 Train Loss:  3.277059555053711, Accuracy 0.6359999775886536\n",
      "epoch:22 Train Loss:  3.2719388961791993, Accuracy 0.6360000371932983\n",
      "epoch:23 Train Loss:  3.266365337371826, Accuracy 0.6359999775886536\n",
      "epoch:24 Train Loss:  3.2612059116363525, Accuracy 0.6360000371932983\n",
      "epoch:25 Train Loss:  3.255393886566162, Accuracy 0.6360000371932983\n",
      "epoch:26 Train Loss:  3.250131893157959, Accuracy 0.6360000371932983\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    train_loss_ep = []\n",
    "    train_acc_ep = []\n",
    "    \n",
    "    for train_batch in dod_df :\n",
    "        train_loss, train_acc = train_step(train_batch, True)\n",
    "        train_loss_ep.append(train_loss)\n",
    "        train_acc_ep.append(train_acc)\n",
    "    if i% 1 == 0:\n",
    "        print(f'epoch:{i} Train Loss:  {np.mean(train_loss_ep)}, Accuracy {np.mean(train_acc_ep)}')\n",
    "    train_loss_t.append(np.mean(train_loss_ep))\n",
    "    train_acc_t.append(np.mean(train_acc_ep))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"~/data/kaggle/dont-overfit/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = DofDataset(df_test, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19750"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(test_ds, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 18s, sys: 7min 41s, total: 22min 59s\n",
      "Wall time: 22min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, batch in enumerate(test_dl):\n",
    "    logits = model(batch)\n",
    "    preds_np = np.argmax(logits.detach().numpy(), axis=1)\n",
    "    preds.extend(preds_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19750"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19750"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samp = pd.read_csv(\"~/data/kaggle/dont-overfit/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samp[\"target\"] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_samp.to_csv(\"subm-2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 144k/144k [00:06<00:00, 21.2kB/s]\n",
      "Successfully submitted to Don't Overfit! II"
     ]
    }
   ],
   "source": [
    "! kaggle competitions submit -c dont-overfit-ii -f subm-2.csv -m \"snn layer nn normalised\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
